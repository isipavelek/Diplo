{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro to neural network -keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Uaaeqkrwse"
      },
      "source": [
        "# Inicia Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB674sYNl7xy"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNVl082Bl7ot"
      },
      "source": [
        "pd.read_csv?"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-MZ8mffHba7"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c-TGLyWHd45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be552f4-5490-4805-ec67-996bceb08aa3"
      },
      "source": [
        "x = np.array(5)\n",
        "x.ndim"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Ak5yWUHdsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80310b7d-b72a-4ba7-f731-cb00f696bee4"
      },
      "source": [
        "x = np.array([5,2,5,9,8])\n",
        "x.ndim"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_csFOeDHw0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2d6968-d313-4a81-dd99-a745e538f9f4"
      },
      "source": [
        "x = np.array([[5,2],[3,1],[2,4],[4,3]])\n",
        "x.ndim"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQPDguReHwpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04de60b3-a807-40da-f9d5-75ec09855e22"
      },
      "source": [
        "x = np.array([[[5,2],[3,1],[2,4],[4,3]],\n",
        "              [[5,2],[3,1],[2,4],[4,3]],\n",
        "              [[5,2],[3,1],[2,4],[4,3]],\n",
        "              [[5,2],[3,1],[2,4],[4,3]]])\n",
        "x.ndim"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34VBXwOpl-Fs"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "\n",
        "# cargamos las 4 combinaciones de las compuertas AND\n",
        "training_data = np.array([[0,0],[0,1],[1,0],[1,1]], \"float32\")\n",
        "\n",
        "# y estos son los resultados que se obtienen, en el mismo orden\n",
        "target_data = np.array([[0],[0],[0],[1]], \"float32\")\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwuT2RdpPzf0"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer='adam',\n",
        "              metrics=['binary_accuracy'])\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGXNTuHgk4uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b591419-e28d-42f7-a056-46d049d32913"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                48        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 65\n",
            "Trainable params: 65\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rIGyOgYmoK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7241a1c-c129-4d56-e71f-2db41205b23d"
      },
      "source": [
        "model.fit(training_data, target_data, epochs=200)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 733ms/step - loss: 0.2506 - binary_accuracy: 0.7500\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2498 - binary_accuracy: 0.7500\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2491 - binary_accuracy: 0.7500\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2483 - binary_accuracy: 0.7500\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2475 - binary_accuracy: 0.7500\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2467 - binary_accuracy: 0.7500\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2459 - binary_accuracy: 0.7500\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2451 - binary_accuracy: 0.7500\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2443 - binary_accuracy: 0.7500\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2436 - binary_accuracy: 0.7500\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2428 - binary_accuracy: 0.7500\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2420 - binary_accuracy: 0.7500\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2413 - binary_accuracy: 0.7500\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2405 - binary_accuracy: 0.7500\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2397 - binary_accuracy: 0.7500\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2390 - binary_accuracy: 0.7500\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2382 - binary_accuracy: 0.7500\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2375 - binary_accuracy: 0.7500\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2367 - binary_accuracy: 0.7500\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2360 - binary_accuracy: 0.7500\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2353 - binary_accuracy: 0.7500\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2346 - binary_accuracy: 0.7500\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2339 - binary_accuracy: 0.7500\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2332 - binary_accuracy: 0.7500\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2325 - binary_accuracy: 0.7500\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2318 - binary_accuracy: 0.7500\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2312 - binary_accuracy: 0.7500\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2305 - binary_accuracy: 0.7500\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2298 - binary_accuracy: 0.7500\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2292 - binary_accuracy: 0.7500\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2285 - binary_accuracy: 0.7500\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2279 - binary_accuracy: 0.7500\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2272 - binary_accuracy: 0.7500\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2265 - binary_accuracy: 0.7500\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2259 - binary_accuracy: 0.7500\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2252 - binary_accuracy: 0.7500\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2246 - binary_accuracy: 0.7500\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2239 - binary_accuracy: 0.7500\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2233 - binary_accuracy: 0.7500\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2226 - binary_accuracy: 0.7500\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2220 - binary_accuracy: 0.7500\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2213 - binary_accuracy: 0.7500\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2207 - binary_accuracy: 0.7500\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2201 - binary_accuracy: 0.7500\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2194 - binary_accuracy: 0.7500\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2188 - binary_accuracy: 0.7500\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2181 - binary_accuracy: 0.7500\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2175 - binary_accuracy: 0.7500\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2168 - binary_accuracy: 0.7500\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2162 - binary_accuracy: 0.7500\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2156 - binary_accuracy: 0.7500\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2149 - binary_accuracy: 0.7500\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2143 - binary_accuracy: 0.7500\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2136 - binary_accuracy: 0.7500\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2130 - binary_accuracy: 0.7500\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2124 - binary_accuracy: 0.7500\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2117 - binary_accuracy: 0.7500\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2111 - binary_accuracy: 0.7500\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2104 - binary_accuracy: 0.7500\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2098 - binary_accuracy: 0.7500\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2092 - binary_accuracy: 0.7500\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2085 - binary_accuracy: 0.7500\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2079 - binary_accuracy: 0.7500\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2072 - binary_accuracy: 0.7500\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2066 - binary_accuracy: 0.7500\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2060 - binary_accuracy: 0.7500\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2053 - binary_accuracy: 0.7500\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2047 - binary_accuracy: 0.7500\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2040 - binary_accuracy: 0.7500\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2034 - binary_accuracy: 0.7500\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2028 - binary_accuracy: 0.7500\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2021 - binary_accuracy: 0.7500\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2015 - binary_accuracy: 0.7500\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2008 - binary_accuracy: 0.7500\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2002 - binary_accuracy: 0.7500\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1996 - binary_accuracy: 0.7500\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1989 - binary_accuracy: 0.7500\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1983 - binary_accuracy: 0.7500\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1976 - binary_accuracy: 0.7500\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1970 - binary_accuracy: 0.7500\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1963 - binary_accuracy: 0.7500\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1957 - binary_accuracy: 0.7500\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1950 - binary_accuracy: 0.7500\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1944 - binary_accuracy: 0.7500\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1937 - binary_accuracy: 0.7500\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1931 - binary_accuracy: 0.7500\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1924 - binary_accuracy: 0.7500\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1918 - binary_accuracy: 0.7500\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1911 - binary_accuracy: 0.7500\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1905 - binary_accuracy: 0.7500\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1898 - binary_accuracy: 0.7500\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1892 - binary_accuracy: 0.7500\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1885 - binary_accuracy: 0.7500\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1879 - binary_accuracy: 0.7500\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1872 - binary_accuracy: 0.7500\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1866 - binary_accuracy: 0.7500\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1860 - binary_accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1853 - binary_accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1847 - binary_accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1840 - binary_accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1834 - binary_accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1827 - binary_accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1821 - binary_accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1814 - binary_accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1808 - binary_accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1801 - binary_accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1795 - binary_accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1788 - binary_accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1782 - binary_accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1775 - binary_accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1769 - binary_accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1763 - binary_accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1757 - binary_accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1752 - binary_accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1746 - binary_accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1741 - binary_accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1735 - binary_accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1729 - binary_accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1723 - binary_accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1717 - binary_accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1711 - binary_accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1705 - binary_accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1698 - binary_accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1692 - binary_accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1686 - binary_accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1680 - binary_accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1675 - binary_accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1669 - binary_accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1664 - binary_accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1659 - binary_accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1654 - binary_accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1648 - binary_accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1643 - binary_accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1637 - binary_accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1632 - binary_accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1627 - binary_accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1622 - binary_accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1616 - binary_accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1611 - binary_accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1605 - binary_accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1599 - binary_accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1594 - binary_accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1589 - binary_accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1584 - binary_accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1579 - binary_accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1574 - binary_accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1569 - binary_accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1563 - binary_accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1558 - binary_accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1553 - binary_accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1547 - binary_accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1542 - binary_accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1537 - binary_accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1532 - binary_accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1527 - binary_accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1522 - binary_accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1517 - binary_accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1512 - binary_accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1507 - binary_accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1501 - binary_accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1496 - binary_accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1491 - binary_accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1486 - binary_accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1481 - binary_accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1476 - binary_accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1471 - binary_accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1466 - binary_accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1461 - binary_accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1456 - binary_accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1451 - binary_accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1446 - binary_accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1441 - binary_accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1436 - binary_accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1431 - binary_accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1426 - binary_accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1421 - binary_accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1417 - binary_accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1412 - binary_accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1407 - binary_accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1402 - binary_accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1397 - binary_accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1392 - binary_accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1387 - binary_accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1382 - binary_accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1378 - binary_accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1373 - binary_accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1368 - binary_accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1363 - binary_accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1358 - binary_accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1353 - binary_accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1349 - binary_accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1344 - binary_accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1339 - binary_accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1334 - binary_accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1329 - binary_accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1324 - binary_accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1320 - binary_accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1315 - binary_accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1310 - binary_accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1305 - binary_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd3963389d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2QgSGWOmKxt"
      },
      "source": [
        "test_data = np.array([[1,0],[1,1],[1,1],[0,0]], \"float32\")\n",
        "\n",
        "# y estos son los resultados que se obtienen, en el mismo orden\n",
        "test_target = np.array([[0],[1],[1],[0]], \"float32\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d15ugdQfb8nW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ccdf87e-4f7a-42a0-c91d-2709661ae74b"
      },
      "source": [
        "scores = model.evaluate(test_data, test_target)\n",
        "\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "print (model.predict(test_data).round())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 127ms/step - loss: 0.1562 - binary_accuracy: 1.0000\n",
            "\n",
            "binary_accuracy: 100.00%\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwSp9LV5mvIW"
      },
      "source": [
        "# Tensorflow!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXyiPm9-QKVj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "outputId": "49827067-b99f-486f-d7a1-78086cb198f7"
      },
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 23 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 71.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.39.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.37.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.6.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tufxqUzIw7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6eb45c-16e9-4712-cebb-639b229ea67f"
      },
      "source": [
        "# coding=utf-8\n",
        "import tensorflow as tf\n",
        "print (tf.VERSION)\n",
        " \n",
        "# input X vector\n",
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "# output Y vector\n",
        "Y = [[0], [0], [0], [1]]\n",
        " \n",
        "# Placeholders for input and output\n",
        "x = tf.placeholder(tf.float32, shape=[4,2])\n",
        "y = tf.placeholder(tf.float32, shape=[4,1])\n",
        " \n",
        "# W matrix\n",
        "W1 = tf.Variable([[1.0, 0.0], [1.0, 0.0]], shape=[2,2])\n",
        "W2 = tf.Variable([[0.0], [1.0]], shape=[2,1])\n",
        " \n",
        "# Biases\n",
        "B1 = tf.Variable([0.0, 0.0], shape=[2])\n",
        "B2 = tf.Variable([0.0], shape=1)\n",
        " \n",
        "# Hidden layer and outout layer\n",
        "output =tf.sigmoid(tf.matmul(tf.sigmoid(tf.matmul(x, W1) + B1), W2) + B2)\n",
        " \n",
        "# error estimation\n",
        "e = tf.reduce_mean(tf.squared_difference(y, output))\n",
        "train = tf.train.GradientDescentOptimizer(0.1).minimize(e)\n",
        " \n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        " \n",
        "for i in range (25001):\n",
        "    error = sess.run(train, feed_dict={x: X, y: Y})\n",
        "    if i % 2000 == 0:\n",
        "        print('\\nEpoch: ' + str(i))\n",
        "        print('\\nError: ' + str(sess.run(e, feed_dict={x: X, y: Y})))\n",
        "        for el in sess.run(output, feed_dict={x: X, y: Y}):\n",
        "            print('    ',el)\n",
        "sess.close()\n",
        " \n",
        "print (\"Complete\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "\n",
            "Epoch: 0\n",
            "\n",
            "Error: 0.32113856\n",
            "     [0.61581236]\n",
            "     [0.61519915]\n",
            "     [0.61519915]\n",
            "     [0.6147866]\n",
            "\n",
            "Epoch: 2000\n",
            "\n",
            "Error: 0.110276505\n",
            "     [0.14202349]\n",
            "     [0.29850715]\n",
            "     [0.29850715]\n",
            "     [0.50733143]\n",
            "\n",
            "Epoch: 4000\n",
            "\n",
            "Error: 0.01955377\n",
            "     [0.02801508]\n",
            "     [0.14485164]\n",
            "     [0.14485164]\n",
            "     [0.8116752]\n",
            "\n",
            "Epoch: 6000\n",
            "\n",
            "Error: 0.007095917\n",
            "     [0.01451061]\n",
            "     [0.08907675]\n",
            "     [0.08907675]\n",
            "     [0.8890776]\n",
            "\n",
            "Epoch: 8000\n",
            "\n",
            "Error: 0.0039386144\n",
            "     [0.01032041]\n",
            "     [0.06689511]\n",
            "     [0.06689511]\n",
            "     [0.9181585]\n",
            "\n",
            "Epoch: 10000\n",
            "\n",
            "Error: 0.0026326082\n",
            "     [0.00826798]\n",
            "     [0.05493741]\n",
            "     [0.05493741]\n",
            "     [0.93347305]\n",
            "\n",
            "Epoch: 12000\n",
            "\n",
            "Error: 0.001944112\n",
            "     [0.00703094]\n",
            "     [0.0473517]\n",
            "     [0.0473517]\n",
            "     [0.94305575]\n",
            "\n",
            "Epoch: 14000\n",
            "\n",
            "Error: 0.001526446\n",
            "     [0.00619365]\n",
            "     [0.0420507]\n",
            "     [0.0420507]\n",
            "     [0.94969195]\n",
            "\n",
            "Epoch: 16000\n",
            "\n",
            "Error: 0.0012489602\n",
            "     [0.00558349]\n",
            "     [0.03810175]\n",
            "     [0.03810175]\n",
            "     [0.9545998]\n",
            "\n",
            "Epoch: 18000\n",
            "\n",
            "Error: 0.0010525244\n",
            "     [0.00511572]\n",
            "     [0.03502607]\n",
            "     [0.03502607]\n",
            "     [0.9584034]\n",
            "\n",
            "Epoch: 20000\n",
            "\n",
            "Error: 0.00090683217\n",
            "     [0.00474351]\n",
            "     [0.03254865]\n",
            "     [0.03254865]\n",
            "     [0.96145135]\n",
            "\n",
            "Epoch: 22000\n",
            "\n",
            "Error: 0.0007948442\n",
            "     [0.00443904]\n",
            "     [0.03050325]\n",
            "     [0.03050325]\n",
            "     [0.9639615]\n",
            "\n",
            "Epoch: 24000\n",
            "\n",
            "Error: 0.00070629246\n",
            "     [0.00418427]\n",
            "     [0.02877861]\n",
            "     [0.02877861]\n",
            "     [0.96607]\n",
            "Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7MzOSfsbf1u"
      },
      "source": [
        "# Pytorch!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FEBGxiSRY-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c997f6e6-292f-48dd-d2cb-e0f4f94b2e75"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class AndNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2,16)\n",
        "        self.fc2 = nn.Linear(16,1)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "m = AndNet()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(m.parameters(), lr=0.001)\n",
        "\n",
        "training_epochs = 3000\n",
        "minibatch_size = 32\n",
        "\n",
        "# input-output pairs\n",
        "pairs = [(np.asarray([0.0,0.0]), [0.0]),\n",
        "         (np.asarray([0.0,1.0]), [0.0]),\n",
        "         (np.asarray([1.0,0.0]), [0.0]),\n",
        "         (np.asarray([1.0,1.0]), [1.0])]\n",
        "\n",
        "state_matrix = np.vstack([x[0] for x in pairs])\n",
        "label_matrix = np.vstack([x[1] for x in pairs])\n",
        "\n",
        "for i in range(training_epochs):\n",
        "        \n",
        "    for batch_ind in range(4):\n",
        "        # wrap the data in variables\n",
        "        minibatch_state_var = Variable(torch.Tensor(state_matrix))\n",
        "        minibatch_label_var = Variable(torch.Tensor(label_matrix))\n",
        "                \n",
        "        # forward pass\n",
        "        y_pred = m(minibatch_state_var)\n",
        "        \n",
        "        # compute and print loss\n",
        "        loss = loss_fn(y_pred, minibatch_label_var)\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            print(i, batch_ind, loss.data)\n",
        "\n",
        "        # reset gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # backwards pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # step the optimizer - update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "print(\"Function after training:\")\n",
        "print(\"f(0,0) = {}\".format(m(Variable(torch.Tensor([0.0,0.0]).unsqueeze(0)))))\n",
        "print(\"f(0,1) = {}\".format(m(Variable(torch.Tensor([0.0,1.0]).unsqueeze(0)))))\n",
        "print(\"f(1,0) = {}\".format(m(Variable(torch.Tensor([1.0,0.0]).unsqueeze(0)))))\n",
        "print(\"f(1,1) = {}\".format(m(Variable(torch.Tensor([1.0,1.0]).unsqueeze(0)))))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 tensor(0.2851)\n",
            "0 1 tensor(0.2797)\n",
            "0 2 tensor(0.2746)\n",
            "0 3 tensor(0.2696)\n",
            "200 0 tensor(0.0003)\n",
            "200 1 tensor(0.0003)\n",
            "200 2 tensor(0.0003)\n",
            "200 3 tensor(0.0003)\n",
            "400 0 tensor(6.1301e-13)\n",
            "400 1 tensor(6.1301e-13)\n",
            "400 2 tensor(6.1301e-13)\n",
            "400 3 tensor(6.2544e-13)\n",
            "600 0 tensor(1.3695e-13)\n",
            "600 1 tensor(1.3695e-13)\n",
            "600 2 tensor(1.4272e-13)\n",
            "600 3 tensor(1.4272e-13)\n",
            "800 0 tensor(3.9246e-14)\n",
            "800 1 tensor(3.9246e-14)\n",
            "800 2 tensor(3.9246e-14)\n",
            "800 3 tensor(3.9246e-14)\n",
            "1000 0 tensor(1.0381e-14)\n",
            "1000 1 tensor(1.0381e-14)\n",
            "1000 2 tensor(6.8279e-15)\n",
            "1000 3 tensor(1.3489e-14)\n",
            "1200 0 tensor(6.0368e-15)\n",
            "1200 1 tensor(2.4841e-15)\n",
            "1200 2 tensor(2.4841e-15)\n",
            "1200 3 tensor(2.4841e-15)\n",
            "1400 0 tensor(1.7070e-15)\n",
            "1400 1 tensor(1.7070e-15)\n",
            "1400 2 tensor(3.2613e-15)\n",
            "1400 3 tensor(3.2613e-15)\n",
            "1600 0 tensor(6.1062e-16)\n",
            "1600 1 tensor(1.6653e-16)\n",
            "1600 2 tensor(1.0547e-15)\n",
            "1600 3 tensor(1.0547e-15)\n",
            "1800 0 tensor(0.)\n",
            "1800 1 tensor(0.)\n",
            "1800 2 tensor(0.)\n",
            "1800 3 tensor(0.)\n",
            "2000 0 tensor(0.)\n",
            "2000 1 tensor(0.)\n",
            "2000 2 tensor(0.)\n",
            "2000 3 tensor(0.)\n",
            "2200 0 tensor(0.)\n",
            "2200 1 tensor(0.)\n",
            "2200 2 tensor(0.)\n",
            "2200 3 tensor(0.)\n",
            "2400 0 tensor(0.)\n",
            "2400 1 tensor(0.)\n",
            "2400 2 tensor(0.)\n",
            "2400 3 tensor(0.)\n",
            "2600 0 tensor(0.)\n",
            "2600 1 tensor(0.)\n",
            "2600 2 tensor(0.)\n",
            "2600 3 tensor(0.)\n",
            "2800 0 tensor(0.)\n",
            "2800 1 tensor(0.)\n",
            "2800 2 tensor(0.)\n",
            "2800 3 tensor(0.)\n",
            "Function after training:\n",
            "f(0,0) = tensor([[0.]], grad_fn=<AddmmBackward>)\n",
            "f(0,1) = tensor([[0.]], grad_fn=<AddmmBackward>)\n",
            "f(1,0) = tensor([[0.]], grad_fn=<AddmmBackward>)\n",
            "f(1,1) = tensor([[1.]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSziV95VR2sH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG91v38qrpjp"
      },
      "source": [
        ""
      ]
    }
  ]
}